---
title: "Mini-Project"
author: "Hazrul"
date: "23 February 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(arm)
library(ggplot2)
library(gridExtra)
library(psych)
library(car)
```

Exploratory Analysis 

```{r cars}
rw_df <- read.csv("RWNS_final.csv", header = TRUE, stringsAsFactors = TRUE)
head(rw_df, 5)
colnames(rw_df)
```



```{r}
summary(rw_df)
```
All predictors are a categorical variable except for IDACI_n. Dependent variable to estimate is ks4score(Age 16 total point score)

A brief description of column names
 
  1. "fiveac"    5 or more GCSE grades A*-C
  2. "fiveem"    5 or more GCSE grades A*-C incle english&maths
  3. "k3en"      score for english (tier) - NC level
  4. "k3ma"      score for math  (tier) - NC level
  5. "k3sc"      Scpre for science (tier) - NC level
  6. "gender"    Male (0) Female (1)
  7. "SECshort"  Social Economic, missing(0), manegerial(1), intermediate(2), unemployed(3)
  8. "hiquamum"  mothers highest educational qualification  (6 Levels)
  9. "singlepar" Single parents no (0), yes(1), -1(missing)
 10. "house"     Ownner occupier rented(0), owned(1), missing/other(-1)
 11. "fsm"       Student entitled to free school meal no(0), yes(1)
 12. "parasp"    Parent wishes YP to continue in FTE

let's create a boxplot to have a first view of the data. 

```{r}
p1 <- ggplot(rw_df, aes(x= fiveem, y=ks4score)) + geom_boxplot()
p2 <- ggplot(rw_df, aes(x= fiveac, y=ks4score)) + geom_boxplot()
p3 <- ggplot(rw_df, aes(x= factor(k3en), y=ks4score)) + geom_boxplot() + labs(x="English Score Tier")
p4 <- ggplot(rw_df, aes(x= factor(k3ma), y=ks4score)) + geom_boxplot() + labs(x="Maths Score Tier")
p5 <- ggplot(rw_df, aes(x= factor(k3sc), y=ks4score)) + geom_boxplot() +  labs(x="Science Score Tier")
p6 <- ggplot(rw_df, aes(x= gender, y=ks4score)) + geom_boxplot()
p7 <- ggplot(rw_df, aes(x= SECshort, y=ks4score)) + geom_boxplot() 
p8 <- ggplot(rw_df, aes(x= hiquamum, y=ks4score)) + geom_boxplot() 
p9 <- ggplot(rw_df, aes(x= singlepar, y=ks4score)) + geom_boxplot()
p10 <- ggplot(rw_df, aes(x= house, y=ks4score)) + geom_boxplot()
p11 <- ggplot(rw_df, aes(x= fsm, y=ks4score)) + geom_boxplot()
p12 <- ggplot(rw_df, aes(x= parasp, y=ks4score)) + geom_boxplot()

grid.arrange(p1, p2, p3, p4, ncol =2, nrow=2)
```
```{r}
p13 <- ggplot(rw_df, aes(x= FSMband, y=ks4score)) + geom_boxplot()
p13
```

```{r}
grid.arrange(p5, p6, p9, p10, p11, p12, ncol =3, nrow=2)
```

```{r}
p7 <- p7 +  theme(axis.text.x = element_text(angle = 30, vjust = 1, hjust=1))
p8 <- p8 + theme(axis.text.x = element_text(angle = 30, vjust = 1, hjust=1))
grid.arrange( p7, p8, ncol =2, nrow=1)
```
```
From boxplot observation, we can detect a few multiconinearity between predictors. we can see that there might exist a colinearity relationships between :
    
    1. fiveem & fiveac
    2. k3en & k3ma & k3sc
    
I think Fiveem & fiveac are dependended predictors. - colinear 
possibly can combine these predictors or omit one of them


#Merging Levels.

This techinique is done by looking at the boxplot where I merge levels that have roughly  the same medians and intuitively make sense to merge them. 

Reduce 3 levels to two, non-professional and professional
```{r}

mani_df <- rw_df

mani_df$new_secshort <- rw_df$SECshort
mani_df$new_secshort <- factor(mani_df$new_secshort, ordered = FALSE)


levels(mani_df$new_secshort)[levels(mani_df$new_secshort) %in% c("Intermediate","Routine,_semi-routine_or_unemployed")] <- "non-professional"

levels(mani_df$new_secshort)[levels(mani_df$new_secshort) %in% c("Managerial_and_professional")] <- "professional"


mani_df$new_secshort <- relevel(mani_df$new_secshort , ref="non-professional")

summary(mani_df$new_secshort)
```


Merge two levels, (GCE_A_Level_or_equivalent,HE_below_degree_level) into a pre-university 
```{r}
mani_df$new_hiquamum <- mani_df$hiquamum

mani_df$new_hiquamum <- factor(mani_df$new_hiquamum, ordered = FALSE)

levels(mani_df$new_hiquamum)[levels(mani_df$new_hiquamum) %in% c("GCE_A_Level_or_equivalent","HE_below_degree_level")] <- "Pre-university-equivalent"

mani_df$new_hiquamum <- relevel(mani_df$new_hiquamum, ref="No_qualification")

summary(mani_df$new_hiquamum)
```

```{r}
missing <- sum(rw_df$singlepar == "missing")
110/13589

```

Since missing values in single parent column only constitues less than 1% (0.8%) and inability to categorise it with any levels, we can remove rows with null values without losing much predictive power.


#Model Selection

Backward Selection
```{r}
model_1 <- lm(ks4score ~ fiveem + fiveac + factor(k3en) + factor(k3ma) + factor(k3sc) + gender + new_secshort + new_hiquamum + singlepar + house + fsm + parasp, data=mani_df)

Anova(model_1)
```

This tells us that as a whole SECshort is the least statistically significant variable. Hence we remove it from our model. 

```{r}
model_2 <- lm(ks4score ~ fiveem + fiveac + factor(k3en) + factor(k3ma) + factor(k3sc) + gender + new_hiquamum + singlepar + house + fsm + parasp, data=mani_df)

Anova(model_2)
```

remove fsm from the model

```{r}
model_3 <- lm(ks4score ~ fiveem + fiveac + factor(k3en) + factor(k3ma) + factor(k3sc) + gender + singlepar + house + parasp + new_hiquamum, data=mani_df)

Anova(model_3)

vif(model_3)
```

remove fsm
```{r}
model_4 <- lm(ks4score ~ fiveem + fiveac + factor(k3en) + factor(k3ma) + factor(k3sc) + gender + singlepar + house + parasp, data=mani_df)

Anova(model_4)

vif(model_4)
```

All predictors are statistical significant at 5% level. 

K3ma and k3sc have high VIF values, 11.67 and 12.9 respectively indicating a problematic amount of colinearity. 
remove k3sc because highly correlated with k3ma
```{r}
model_5 <- lm(ks4score ~ fiveem + fiveac + factor(k3en) + factor(k3ma) + gender + singlepar + house + parasp, data=mani_df)

vif(model_5)
```

remove k3ma and fiveem because both are highly correlated to fiveac and k3ma respectively
```{r}

model_6 <- lm(ks4score ~ fiveac + factor(k3en) + gender + singlepar + house + parasp, data=mani_df)

vif(model_6)

display(model_6)

Anova(model_6)
```

# Final Model
Include two variables that are statistically significant from the other subset of predictors
```{r}
model_7 <- lm(ks4score ~ fiveac + k3ma + gender + singlepar  + attitude + exclude, data=mani_df)

Anova(model_7)
vif(model_7)
summary(model_7)
```

Comparing two predictors that are potentially dependent to one another. (can include interactions) 
```{r}
model_8 <- lm(ks4score ~ fiveem, data=mani_df)
display(model_8)
```

```{r}
model_8.1 <- lm(ks4score ~ fiveac, data=mani_df)
display(model_8.1)
```

The coefficient is different but 

The fact that adding fiveem to the model leads to a marginally tiny increase in R-square statistics (0.5718 -> 0.5957) shows an evidence that fiveem can be droped from the model. Fiveem provides no real improvement in the model fit and its inclusion will likely lead to poor performance. but this project is about finding which predictor affects the target variable. 


# Looking at the missing values.
```{r}
mani_df$singlepar <- factor(mani_df$singlepar, ordered = FALSE)

mani_df$singlepar <- relevel(mani_df$singlepar, ref="yes")
model_8 <- lm(ks4score ~ singlepar, data=mani_df)

summary(model_8)
```

Missing values
the p-values associated with the coefficient estimate for missing variable are statistically non significant as it is above 5% level, suggesting no statistical evidence of a real difference in average grade between those with single parent and those who dont delcare. There might be a relationship of people with single parents are unlikely to declare that they have a single parent because of shame, etc.

# Backward Selection using all predictors
```{r}
model_9 <- lm(ks4score ~ fiveem + fiveac + factor(k3en) + factor(k3ma) + factor(k3sc) + gender + new_secshort + new_hiquamum + singlepar + house + fsm + parasp + computer + tuition + pupasp + homework + attitude + sen + truancy + absent + exclude + IDACI_n + FSMband, data =mani_df)

Anova(model_9)
```

fsm is the least statistically significant predicors. Hence we omit it. We'll do this process untill all predictors are statstically significant at 5% level. 

```{r}
model_10 <- lm(ks4score ~ fiveem + fiveac + factor(k3en) + factor(k3ma) + factor(k3sc) + gender + new_secshort+ new_hiquamum + singlepar + house + parasp + computer + tuition + pupasp + homework + attitude + sen + truancy + absent + exclude + IDACI_n + FSMband, data =mani_df)

Anova(model_10)
```

new_secshort is removed. 
```{r}
model_11 <- lm(ks4score ~ fiveem + fiveac + factor(k3en) + factor(k3ma) + factor(k3sc) + gender + new_hiquamum + singlepar + house + parasp + computer + tuition + pupasp + homework + attitude + sen + truancy + absent + exclude + IDACI_n + FSMband, data =mani_df)

Anova(model_11)
```

tutition is removed. 
```{r}
model_12 <- lm(ks4score ~ fiveem + fiveac + factor(k3en) + factor(k3ma) + factor(k3sc) + gender + new_hiquamum + singlepar + house + parasp + computer + pupasp + homework + attitude + sen + truancy + absent + exclude + IDACI_n + FSMband, data =mani_df)

Anova(model_12)
```

new_hiquamum is removed
```{r}
model_13 <- lm(ks4score ~ fiveem + fiveac + factor(k3en) + factor(k3ma) + factor(k3sc) + gender + singlepar + house + parasp + computer + pupasp + homework + attitude + sen + truancy + absent + exclude + IDACI_n + FSMband, data =mani_df)

Anova(model_13)
```

IDACI_n is removed
```{r}
model_14 <-lm(ks4score ~ fiveem + fiveac + factor(k3en) + factor(k3ma) + factor(k3sc) + gender + singlepar + house + parasp + computer + pupasp + homework + attitude + sen + truancy + absent + exclude + FSMband, data =mani_df)

Anova(model_14)
```

parasp is removed
```{r}
model_15 <- lm(ks4score ~ fiveem + fiveac + factor(k3en) + factor(k3ma) + factor(k3sc) + gender + singlepar + house + computer + pupasp + homework + attitude + sen + truancy + absent + exclude + FSMband, data =mani_df)

Anova(model_15)
vif(model_15)
```

```{r}
corPlot(mani_df[,4:6])
```

based on boxplot and vif
  
  1. fiveem and fiveac are correlated.
  2. kc3ma, kc3en, kcsc are correlated.
  3. absent and exclude are correlated.

```{r}
model_16 <- lm(ks4score ~ fiveac + k3ma + gender + singlepar + house + computer + pupasp + homework + attitude + sen + truancy + absent + FSMband, data =mani_df)

Anova(model_16)

vif(model_16)
```


```{r}
summary(model_16)
```


```{r}
```


I just a run backward selection again but this time on all predictors. I ended up with this model. 
s4score ~ fiveac + k3ma + gender + singlepar + house + computer + pupasp + homework + attitude + sen + truancy + absent + FSMband. 

There's still a lot more to do like merging some levels of some categorical variables. 

































